---
title: "From COMET to COMES – Can Summary Evaluation Benefit from Translation Evaluation?"
collection: publications
excerpt: ''
authors: "**Mateusz Krubiński**, Pavel Pecina"
venue: 'The 3rd Workshop on Evaluation & Comparison of NLP Systems (Eval4NLP @ AACL-IJCNLP 2022)'
paperurl: 'https://aclanthology.org/2020.wmt-1.16.pdf'
---

[**[Paper]**](https://aclanthology.org/2020.wmt-1.16.pdf)

COMET is a recently proposed trainable neural-based evaluation metric developed to assess the quality of Machine Translation systems. In this paper, we explore the usage of COMET for evaluating Text Summarization systems – despite being trained on multilingual MT outputs, it performs remarkably well in monolingual settings, when predicting summarization output quality. We introduce a variant of the model – COMES – trained on the annotated summarization outputs that uses MT data for pre-training. We examine its performance on several datasets with human judgments collected for different notions of summary quality, covering several domains and languages.
